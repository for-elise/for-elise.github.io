<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="assets/css/main.css">
    <title>DexCap | Scalable and Portable Mocap Data Collection System for Dexterous Manipulation</title>

</head>
<body>

<div id="title_slide">
    <div class="title_left">
        <h1>DexCap: Scalable and Portable Mocap Data<br>Collection System for Dexterous Manipulation</h1>
        <div class="author-container">
            <div class="author-name"><a href="https://www.chenwangjeremy.net/" target="_blank">Chen Wang</a></div>
            <div class="author-name"><a href="https://hshi74.github.io/" target="_blank">Haochen Shi</a></div>
            <div class="author-name"><a href="https://tml.stanford.edu/people/weizhuo-ken-wang" target="_blank">Weizhuo Wang</a></div>
            <div class="author-name"><a href="https://ai.stanford.edu/~zharu" target="_blank">Ruohan Zhang</a></div>
            <div class="author-name"><a href="https://profiles.stanford.edu/fei-fei-li" target="_blank">Li Fei-Fei</a></div>
            <div class="author-name"><a href="https://profiles.stanford.edu/c-karen-liu" target="_blank">C. Karen Liu</a></div>
        </div>
        <div class="affiliation">
            <p><img src="assets/logos/SUSig-red.png" style="height: 50px"></p>
        </div>
        <div class="button-container">
            <a href="assets/DexCap_paper.pdf" target="_blank" class="button"><i class="fa-light fa-file"></i>&emsp14;PDF</a>
            <a href="https://arxiv.org/abs/2403.07788" target="_blank" class="button"><i class="ai ai-arxiv"></i>&emsp14;arXiv</a>
            <a href="https://youtu.be/-PmjRjgXKuo" target="_blank" class="button"><i class="fa-light fa-film"></i>&emsp14;Video</a>
            <a href="https://twitter.com/chenwang_j/status/1768304378577084739" target="_blank" class="button"><i class="fa-brands fa-x-twitter"></i>&emsp14;tl;dr</a>
            <a href="https://github.com/j96w/DexCap" target="_blank" class="button"><i class="fa-light fa-code"></i>&emsp14;Code</a>
            <a href="https://huggingface.co/datasets/chenwangj/DexCap-Data" target="_blank" class="button"><i class="fa-light fa-face-smiling-hands"></i>&emsp14;Data</a>
            <a href="https://docs.google.com/document/d/1ANxSA_PctkqFf3xqAkyktgBgDWEbrFK7b1OnJe54ltw/edit?usp=sharing" target="_blank" class="button"><i class="fa-light fa-robot-astromech"></i>&emsp14;Hardware</a>
        </div>

        <br>
        <div class="slideshow-container">
            <div class="video_container">
                <video loop autoplay muted playsinline preload="metadata" width="100%">
                    <source src="assets/web_video_first_c.mp4" type="video/mp4">
                </video>
            </div>
        </div>
        <br>


        <div id="abstract">
            <h1>Abstract</h1>
            <p>
                Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks.
                Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems
                and the difficulty of translating mocap data into effective control policies. To tackle these issues, we introduce DexCap, a portable
                hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand
                mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together
                with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning
                to replicate human actions with robot hands. Beyond learning from human motion, DexCap also offers an optional human-in-the-loop correction mechanism
                to refine and further improve robot performance. Through extensive evaluation across six dexterous manipulation tasks, our approach not only demonstrates
                superior performance but also showcases the system's capability to effectively learn from in-the-wild mocap data, paving the way for future data collection
                methods for dexterous manipulation.
            </p>
        </div>
    </div>
</div>
<hr class="rounded">
<div id="overview">

    <h1>DexCap: A Portable Hand Motion Capture System</h1>

    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/system.mp4" type="video/mp4">
            </video>
        </div>
    </div>
    <p>
        Overview of the DexCap system:
        <br><b>Front design:</b> A camera rack on the chest is equipped with an RGB-D LiDAR camera and three SLAM tracking cameras.
        <br><b>Back design:</b> A mini-PC and power bank in the backpack power the system for approximately 40 minutes of data collection.
        <br><b>Data collection process:</b> The tracking cameras, initially placed in the camera rack for calibration,
        are relocated to hand mounts during data collection to consistently track the palm positions.
        Finger motions are captured by motion capture gloves.
    </p>

    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/data_vis.mp4" type="video/mp4">
            </video>
        </div>
    </div>
    <p>
        <b>Data visualization:</b> 3D hand motion capture data in point cloud observation.
    </p>

    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/speed_comp.mp4" type="video/mp4">
            </video>
        </div>
    </div>
    <p>
        <b>Data collection throughput:</b> DexCap is about three times faster than teleoperation in data collection throughput and is close to the level of natural human motion.
    </p>

    <div class="allegrofail" style="text-align: center;">
        <div class="video_container" style="display: inline-block;">
            <video autoplay muted playsinline loop preload="metadata" width="70%">
                <source src="assets/comp_vr_c.mp4" type="video/mp4">
            </video>
        </div>
    </div>
    <p>
        <b>Compare to vision-based method:</b> In this example, human is holding the mug handle with a fixed gesture. The vision-based hand tracking method used by the VR headset fails to accurately track
                    finger positions due to heavy occlusion. DexCap is more capable of collecting hand-object interaction data.
    </p>


    <h1>From Human to Robot</h1>
    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/human_to_robot.mp4" type="video/mp4">
            </video>
        </div>
    </div>
    <p>
        <b>Observation retargeting:</b> To simplify the process of switching the camera system between the human and robot,
        a quick-release buckle has been integrated into the back of the camera rack, allowing for swift camera swaps
        – in less than 20 seconds. In this way, the robot utilizes the same observation camera employed during human data collection.
    </p>
    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/fingertip_ik.mp4" type="video/mp4">
            </video>
        </div>
    </div>
    <p>
        <b>Action retargeting:</b> To transfer human finger motion to the LEAP robot hand, we use fingertip
        inverse kinematics (IK) to compute the 16-dimensional joint positions. Human finger motions are tracked
        using a pair of motion capture gloves, which measure the 3D positions of the fingers relative to the palm based on electromagnetic field (EMF).
    </p>

    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/dataset.mp4" type="video/mp4">
            </video>
        </div>
    </div>
    <p>
        <b>Visual gap:</b> To further bridge the visual gap between human hand and robot hand,
        we use forward kinematics to genrate a point cloud mesh of the robot hand and add it to the pointcloud observation as is shown in this video.
    </p>

    <br>
    <br>
    <br>
    <hr class="rounded">

    <h1>Method: Data Retargeting and Imitation Learning</h1>
    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/method2.mp4" type="video/mp4">
            </video>
        </div>
    </div>
    <p>
        We first retarget the DexCap data to the robot embodiment by constructing 3D point clouds from RGB-D observations and transforming it into robot operation space.
        Meanwhile, the hand motion capture data is retargeted to the dexterous hand and robot arm with fingertip IK.
        Based on the data, a Diffusion Policy is learned to take the point cloud as input and outputs a sequence of future goal positions as the robot actions.
    </p>

    <h1>Results</h1>
    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/normal_ball.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>Fully autonomous policy rollouts. Policy learned with 30-minute human mocap data without any teleoperation.</p>
            </div>
        </div>
    </div>

    <h1>Bimanual Manipulation Task</h1>
    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/normal_bimaual.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>0:00-0:09 Collecting bimanual human mocap data <br>0:10-1:47 Fully autonomous policy rollouts (learned with 30-minute human mocap data without any teleoperation)</p>
            </div>
        </div>
    </div>

    <br>
    <hr class="rounded">

    <h1>In-the-wild Data Collection with DexCap</h1>
    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/system_in_wild.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    <div class="allegrolower">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/transfer_c.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p><b>Transfer to robot space</b></p>
            </div>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/dataset_c.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p><b>Remove redundant points and add point clouds of the robot hand</b></p>
            </div>
        </div>
    </div>

    <h1>Policy learned with In-the-wild DexCap Data</h1>
    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/normal_packaging_trainedobj.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p><b>Trained objects:</b> Fully autonomous policy rollouts in 1x speed.</p>
            </div>
        </div>
    </div>

    <div class="allegrolower">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/normal_packaging_unseenobj.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p><b>Unseen objects:</b>. Fully autonomous policy rollouts in 1x speed.</p>
            </div>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/normal_packaging_unseenobj2.mp4" type="video/mp4">
            </video>
        </div>
    </div>


    <br>
    <hr class="rounded">

    <h1>Human-in-the-loop correction with DexCap</h1>
    <br>
    <div class="allegrolower">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/HIL_mode1.mp4" type="video/mp4">
            </video>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/HIL_mode2.mp4" type="video/mp4">
            </video>
        </div>
    </div>
    <p>
        DexCap supports two types of human-in-the-loop correction during the policy rollouts: <br>
        <b>(1). Residual correction</b> measures the 3D delta position changes of the human wrist and incorporates them as residual actions to the robot's wrist movements.
        This mode enables minimal movement but requiring more precise control.<br>
        <b>(2). Teleoperation</b> directly translates full human hand motions to the robot end-effector actions based on inverse kinematics.
        This mode enables the full control over the robot but requiring more effort.<br>
        Users can switch between the two modes by stepping on the foot pedal during the rollouts.
    </p>

    <div class="video_container">
        <img src="assets/HIL_method.png" alt="Description of Image">
    </div>
    <p>
        The corrections are stored in a new dataset and uniformly sampled with the original dataset for fine-tuning the robot policy
    </p>

    <h1>Results after finetuning - Tea preparing</h1>
    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/normal_tea.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>Fully autonomous policy rollouts in 2x speed. Policy learned with 1-hour human mocap data and 30 human-in-the-loop corrections.</p>
            </div>
            <br>
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/normal_tea_more.mp4" type="video/mp4">
            </video>
        </div>
    </div>


    <h1>Results after finetuning - Scissor cutting</h1>
    <div class="allegrofail">
        <div class="video_container">
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/normal_scissor.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>Fully autonomous policy rollouts in 2x speed. Policy learned with 1-hour human mocap data and 30 human-in-the-loop corrections.</p>
            </div>
            <br>
            <video autoplay muted playsinline loop controls preload="metadata">
                <source src="assets/normal_scissor_more.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    <hr class="rounded">
    <h1>Acknowledgments</h1>
    <p>
        This research was supported by National Science Foundation NSF-FRR-2153854 and Stanford Institute
        for Human-Centered Artificial Intelligence, SUHAI. This work is partially supported by ONR MURI N00014-21-1-2801.
        We would like to thank Yunfan Jiang, Albert Wu, Paul de La Sayette, Ruocheng Wang, Sirui Chen, Josiah Wong, Wenlong Huang,
        Yanjie Ze, Christopher Agia, Jingyun Yang and the SVL PAIR group for providing help and feedback. We also thank Zhenjia Xu, Cheng Chi,
        Yifeng Zhu for their suggestions in tuning the robot controller. We especially thank Kenneth Shaw, Ananye Agrawal, Deepak Pathak for open-sourcing the LEAP Hand.
    </p>

    <br>
    <br>
    <hr class="rounded">
    <h1>BibTeX</h1>
    <p class="bibtex">
        @inproceedings{wang2024dexcap, <br>
        &nbsp;&nbsp;&nbsp;&nbsp;title = {FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance}, <br>
        &nbsp;&nbsp;&nbsp;&nbsp;author = {Ruocheng Wang and Pei Xu and Haochen Shi and Elizabeth Schumann and C. Karen Liu}, <br>
        &nbsp;&nbsp;&nbsp;&nbsp;booktitle = {SIGGRAPH Asia 2024}, <br>
        &nbsp;&nbsp;year = {2024} <br>
        }
    </p>
    <br>
    <br>
</div>
</body>

<script src="assets/js/full_screen_video.js"></script>
<script src="assets/js/carousel.js"></script>
</html>
